---
title: "Homework Unit 11: Model Comparisons and Other Explanatory Goals"
author: "Nicole Friedl"
date: "`r lubridate::today()`"
format: 
  html: 
    embed-resources: true
    toc: true 
    toc_depth: 4
editor_options: 
  chunk_output_type: console
---


## Introduction

To begin, download the following from the course web book (Unit 11):

* `hw_unit_11_explanatory.qmd` (notebook for this assignment)

* `student_perf_cln.csv` (data for this assignment)

The data for this week's assignment have information about students and their final grade performance in a math class.  

The data will be cleaned using code included for you below, and you don't need to show any modeling EDA to keep the assignment from getting too long. *However, you should still always be checking your data as you progress through the assignment!*

In this assignment, you will practice comparing models to determine the importance of a pair of focal variables. In particular, you will be determining whether parental education level (`mother_educ` and `father_educ`) matter for predicting student performance. You will then use the Bayesian approach to evaluate the effect of that focal variable. 

We will be using 30 splits (3 repeats of 10 fold cv) to tune and select the best GLM model configuration for two feature sets: a full model (with all of the predictors) and a compact model (with all predictors except the two focal variables: `mother_educ` and `father_educ`). We will also be introducing a new performance metric: **R-squared**. Yay!    

Let's get started!

-----


##  Setup

Set up your notebook in this section.

```{r}
options(conflicts.policy = "depends.ok")
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true")
tidymodels_conflictRules()

library(tidyverse)
library(magrittr, exclude = c("set_names", "extract"))
library(rsample)
library(parsnip)
library(recipes)
library(workflows)


devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_plots.R?raw=true")
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true")
options(tibble.width = Inf, dplyr.print_max=Inf)
rerun_setting <- FALSE

cl <- parallel::makePSOCKcluster(parallel::detectCores(logical = FALSE))
doParallel::registerDoParallel(cl)

path_data <- "homework/data"
```



## Read in your data

Read in the `student_perf_cln.csv` data file as `data_all`. Class variables as appropriate.    

Note the following levels for your focal variables (`mother_educ` and `father_educ`) are:   

0 - none    
1 - primary education (4th grade)     
2 – 5th to 9th grade    
3 – secondary education   
4 – higher education   

```{r}
data_all <- read_csv(here::here(path_data, "student_perf_cln.csv"),
                     show_col_types = FALSE) |> 
  glimpse()
```

## Set up splits

Split `data_all` into repeated k-fold cross-validation splits using 3 repeats and 10 folds. Save your splits as an object named `splits` 

```{r}
set.seed(123456)

splits <- vfold_cv(data_all, v = 10, repeats = 3)

splits
```

## Build recipes

We will be fitting GLM models tuned on `penalty` and `mixture` for two feature sets: one for a "full" model (contains all predictors), and one for a compact model (contains all predictors except the two focal variables: `mother_educ` and `father_educ`). 

### Recipe 1: Full model

Create a recipe for setting up features for your full model.

```{r}
rec_full <- recipe(grade ~ ., data = data_all) |> 
  step_dummy(all_nominal_predictors()) |> 
  step_normalize(all_numeric_predictors())
```

### Recipe 2: Compact model

Because your recipe for the compact model will only differ from your recipe for the full model by one step (removing your focal variables), you can start with `rec_full`. Then add a step that will remove the *dummy coded* focal variables `father_educ` and `mother_educ`. Remember you can make a feature matrix to see what these dummy coded variables end up being named!

```{r}
rec_compact <- rec_full |> 
  step_rm(mother_educ, father_educ)
```

## Fit models

### Set up a hyperparameter tuning grid
```{r}
tune_grid <- expand_grid(
  penalty = exp(seq(-8, 2, length.out = 300)),
  mixture = c(0, .025, .05, .1, .2, .4, .6, .8, 1)
)
```


### Fit the full model

Use `rec_full`, `splits`, and `tune_grid` to fit GLM's across folds.  Use R squared (`rsq`) as your metric. Save your model fits as `fits_full`. 
```{r}
library(tune)
library(yardstick)

fits_full <- 
  linear_reg(penalty = tune(),
             mixture = tune()) |> 
  set_engine("glmnet") |> 
  tune_grid(
    preprocessor = rec_full,
    resamples = splits,
    grid = tune_grid,
    metrics = metric_set(rsq)
  )
```

Make sure you considered a good range of hyperparameter values
```{r}
plot_hyperparameters(tune_fit = fits_full, 
                     hp1 = "penalty", 
                     metric = "rsq", 
                     log_hp1 = TRUE)

plot_hyperparameters(tune_fit = fits_full, 
                     hp1 = "penalty", 
                     hp2 = "mixture", 
                     metric = "rsq", 
                     log_hp1 = TRUE)
```

Select best model configuration
```{r}
best_full <- select_best(fits_full, metric = "rsq")
print(best_full)
```

Print the mean R squared of the best model configuration across the 30 held-out folds.
```{r}
collect_metrics(fits_full, summarize = TRUE) |> 
  filter(.config == select_best(fits_full, metric = "rsq")$.config)
```

### Fit the compact model

Use `rec_compact`, `splits`, and `tune_grid` to fit GLM's across folds. Use R-squared as your metric. Save your model fits as `fits_compact`. 
```{r}
fits_compact <- 
  linear_reg(penalty = tune(),
             mixture = tune()) |> 
  set_engine("glmnet") |> 
  tune_grid(
    preprocessor = rec_compact,
    resamples = splits,
    grid = tune_grid,
    metrics = metric_set(rsq)
  )
```

Select best model configuration
```{r}
best_compact <- select_best(fits_compact, metric = "rsq")
print(best_compact)
```

Print the mean R squared of the best model configuration across the 30 held-out folds.
```{r}
collect_metrics(fits_compact, summarize = TRUE) |> 
  filter(.config == select_best(fits_compact)$.config)
```

## Model comparison with the Bayesian approach
You will now compare your models using the Bayesian parameter estimation

### Gather performance estimates

First, make a data frame containing the 30 performance estimates from held out folds for your full and compact model. Hint you can use the code above but change `summarize = TRUE` to `summarize = FALSE`. Filter down so you only have the following variables (`id`, `id2`, `.estimate`). Rename `.estimate` to `full` or `compact` before joining your estimates.
```{r}
hp_best_full <- select_best(fits_full, metric = "rsq")
hp_best_compact <- select_best(fits_compact, metric = "rsq")

library(tidyposterior)

accuracy_full <- collect_metrics(fits_full, summarize = FALSE) |> 
  filter(.config == hp_best_full$.config) |>
  select(id, id2, .estimate) |>
  rename(full = .estimate)

accuracy_compact <- collect_metrics(fits_compact, summarize = FALSE) |> 
  filter(.config == hp_best_compact$.config) |>
  select(id, id2, .estimate) |>
  rename(compact = .estimate)

resamples <- accuracy_full |> 
  full_join(accuracy_compact, by = c("id", "id2"))

head(resamples)
```


### Posterior probabilities

Next, derive the posterior probabilities for R squared of each of these two models
```{r}
set.seed(101)

pp <- perf_mod(
  resamples, 
  formula = statistic ~ model + (1 | id2/id),
  iter = 2000,
  chains = 4,
  transform = tidyposterior::logit_trans, 
  hetero_var = TRUE,
  family = gaussian,
  adapt_delta = .99
)
```

### Graph positerior probabilities

Display your posterior probabilities using both a density plot and a histogram. Choose plots that would be the most useful to display your results to a collaborator.
```{r}
ggplot(
  tidy(pp), 
  aes(x = posterior, fill = model)
) +
  geom_density(alpha = 0.7) +
  labs(
    title = "Posterior Distributions of R-squared",
    subtitle = "Comparing Full Model (with parent education) vs. Compact Model",
    x = "R-squared",
    y = "Density"
  ) +
  theme_minimal() +
  scale_fill_brewer(palette = "Set1")

ggplot(
  tidy(pp), 
  aes(x = posterior, fill = model)
) +
  geom_histogram(alpha = 0.7, position = "identity", bins = 30) +
  labs(
    title = "Histogram of Posterior R-squared Values",
    subtitle = "Comparing Full Model (with parent education) vs. Compact Model",
    x = "R-squared",
    y = "Count"
  ) +
  theme_minimal() +
  scale_fill_brewer(palette = "Set1")

ggplot(
  tidy(pp), 
  aes(x = model, y = posterior, fill = model)
) +
  geom_violin(alpha = 0.7) +
  geom_boxplot(width = 0.1, alpha = 0.5) +
  labs(
    title = "Violin Plot of Posterior R-squared Values",
    subtitle = "Comparing Full Model vs. Compact Model",
    y = "R-squared"
  ) +
  theme_minimal() +
  scale_fill_brewer(palette = "Set1")
```

### Determine if the full model has better performance

Calculate the probability that the full model is performing better than the compact model. 

```{r}
model_contrast <- contrast_models(pp, seed = 123)

summary_with_rope <- summary(model_contrast, size = 0.01)
print(summary_with_rope)

summary_no_rope <- summary(model_contrast, size = 0)
print(summary_no_rope)

autoplot(model_contrast) +
  labs(
    title = "Difference in Performance Between Full and Compact Models",
    subtitle = "Positive values indicate the full model performs better",
    x = "Difference in R-squared"
  ) +
  theme_minimal()
```

Type out a summary of what you find that includes:

* Your interpretation of what the mean increase in R  squared and 95% HDI values represent

* Your conclusion if the full model is meaningfully better than your compact model and why   

*The full model's R-squared is, on average, a tiny -0.00119 lower than the compact model, with a 95% HDI of -0.0116 to 0.00961.  With a ROPE of 0.01, they're practically equivalent 87.6% of the time.  So, the full model isn't meaningfully better; the compact model is simpler and just as good.*

## Feature importance

You will now look at feature importance in your full model using Shapely Values.

### Prep data
Get your data ready for use with the `DALEX` package. Do the following:    

Create a feature matrix from the whole data set

```{r}
library(DALEX, exclude = "explain")
library(DALEXtra)

rec_full_prep <- rec_full |>  
  prep(data_all)

feat_full <- rec_full_prep |>  
  bake(data_all)

best_params_full <- select_best(fits_full, metric = "rsq")

fit_all_data <- 
  linear_reg(penalty = best_params_full$penalty,
             mixture = best_params_full$mixture) |>  
  set_engine("glmnet") |>  
  fit(grade ~ ., data = feat_full)
```


Pull out your features (`x`) and outcome (`y`) to be used in calculating feature importance

```{r}
x <- feat_full |>  select(-grade)
y <- feat_full |>  pull(grade)
```


Use the following code to define your predictor function (`predict_wrapper`) and explainer object (`explain_full`).
```{r}
predict_wrapper <- function(model, newdata) {
  predict(model, newdata) |>  
    pull(.pred)
}

explain_full <- explain_tidymodels(fit_all_data, 
                                   data = x, 
                                   y = y, 
                                   predict_function = predict_wrapper)
```

### Calculate Shapely Values for a single participant

First, we will look at shapely values for a single participant. For this example, lets look at the last participant in our data set. Do the following:

* Print the raw feature values for the last participant. 

* Generate Shapely Values for this participant (you might consider caching this!)

* Use `ggplot()` to plot the Shapely Values for this participant

```{r}
last_obs_index <- nrow(x)
x_last <- x |>  
  slice(last_obs_index) |>  
  glimpse()

set.seed(123456)
sv_last <- predict_parts(explain_full, 
                         new_observation = x_last,
                         type = "shap",
                         B = 25)

sv_last |> 
  filter(B == 0) |> 
  mutate(variable = fct_reorder(variable_name, contribution)) |> 
  ggplot(aes(x = contribution, y = variable)) +
  geom_col(fill = "steelblue") +
  labs(
    title = "Shapley Values for Last Participant",
    subtitle = "How each feature contributes to the prediction",
    x = "Contribution to prediction",
    y = "Feature"
  ) +
  theme_minimal()
```



### Calculate mean absolute Shapely Values across all participants

Now we will look at feature importance across all participants. Since this process is extremely computationally intensive, we are going to select a random set of participants to demonstrate this on.    

Include the following steps:    

* Use the function `slice_sample()` with `n` set to 20 to get a random subset of observations from `x`. Don't forget to set a seed first. Remember you can type `?slice_sample()` into your console to learn more about a function. 

* Calculate Shapely Values for each observation (in your reduced feature set) and `glimpse()` the resulting tibble

* Plot the mean absolute Shapely Values across participants

```{r}
get_shaps <- function(df1) {
  predict_parts(explain_full, 
                new_observation = df1,
                type = "shap",
                B = 25) |>  
    filter(B == 0) |>  
    select(variable_name, variable_value, contribution) |>  
    as_tibble()
}

set.seed(123456)
x_sample <- x |> 
  slice_sample(n = 20)

set.seed(123456)
local_shaps <- x_sample |> 
  mutate(id = row_number()) |> 
  nest(.by = id, .key = "dfs") |> 
  mutate(shaps = map(dfs, \(df1) get_shaps(df1))) |>  
  select(-dfs) |> 
  unnest(shaps)

glimpse(local_shaps)

local_shaps |> 
  mutate(contribution = abs(contribution)) |> 
  group_by(variable_name) |> 
  summarize(mean_shap = mean(contribution)) |> 
  arrange(desc(mean_shap)) |> 
  mutate(variable_name = factor(variable_name),
         variable_name = fct_reorder(variable_name, mean_shap)) |> 
  ggplot(aes(x = variable_name, y = mean_shap)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(
    title = "Mean Absolute Shapley Values Across Participants",
    subtitle = "Higher values indicate more important features",
    x = "",
    y = "Mean |Shapley Value|"
  ) +
  theme_minimal()
```

Write a brief description of your top takeaways from this plot.    

*Basically, the full model edges out the compact model, but it's a tiny, uncertain gain, mostly around zero.  They're pretty much neck and neck.*





