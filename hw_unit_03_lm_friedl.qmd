---
title: "Homework Unit 3: LM"
author: "Nicole Friedl"
date: "`r lubridate::today()`"
format: 
  html: 
    embed-resources: true
    toc: true 
    toc_depth: 4
editor_options: 
  chunk_output_type: console
---

## General Instructions 
Welcome to the LM portion of your Unit_3 homework! Please enter your first and last name at the top of this document where it says "Your name here".

This homework aligns with **Unit 3 Exploratory Introduction to Regression Models** in the course web book, where you can download all materials required for this assignment:

**Two Markdown files (`hw_unit_3_lm.qmd` and `hw_unit_3_knn.qmd`):** 
We will have separate files for fitting lm and knn models for this homework. The order you complete these in does not matter; both contain all instructions needed for the assignment.

- **`ames_train_cln.csv`**:
The `.csv` file containing the cleaned training data to be used for this assignment.

- **`ames_val_cln.csv`**:
The `.csv` file containing the cleaned validation data to be used for this assignment.

- **`ames_test_cln.csv`**:
The `.csv` file containing the cleaned test data to be used for this assignment. `sale_price` has been removed from this file.

- **`ames_data_dictionary.pdf`:**
The data dictionary describing variables contained in the data set.

-----

The goal of this assignment is to create the best regression model you can to predict housing sale prices in the Ames data set. To build this model, you will have all variables used by John in the unit 2 & 3 web book to demo EDA and Regression Models, as well as all variables that you performed EDA on yourself in the unit 2 homework assignment.

A very nice TA combined the EDA cleaning steps from the assignment and web book data to provide you with three data files (training, validation, and test) containing all cleaned variables that you need for this assignment.

Using `ames_train_cln` as your starting point, your job is to use simple feature engineering techniques (e.g. selecting among variables, transforming variables, coding categorical variables, adding interactions) to iteratively train and compare multiple `lm` and `knn` models. **This script is for training `lm` models.** You do not need to perform any additional cleaning EDA, but you may continue to add on to your homework 2 EDA modeling script as you consider different feature engineering approaches. You do not need to turn in any additional modeling EDA code you complete.

At the end of this assignment, you will select your best performing model across all `lm` and `knn` model configurations you fit. You will use `ames_test_cln` *only once* at the end of the assignment to generate your best model's predictions of `sale_price` in the held out test set. The TAs will assess everyone's predictions on the held-out set to determine the best fitting model in the class.  **The winner gets a FREE LUNCH with John (lucky you!! ;-)**

Lets get started!

## Set up

### Handle conflicts
```{r}
#| message: false

options(conflicts.policy = "depends.ok")
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true")
tidymodels_conflictRules()

```

### Load required packages and source

Add packages (only what you need!)
```{r}
library(tidyverse)
library(tidymodels)
library(recipes)
```

Source any function scripts (John's are already sourced for you, but you can change these or add your own function scripts too!)
```{r}
#| message: false

devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_plots.R?raw=true")
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true")
```

### Specify other global settings
```{r settings}
#| include: false

theme_set(theme_classic())
options(tibble.width = Inf, dplyr.print_max=Inf)
```

### Paths

```{r path}
path_data <- "homework/data"
```

### Load data

Load the cleaned training, validation, and test data files. **Please load your data using the filenames provided above**

Use `here::here()` and relative path for your data.  Make sure your iaml project is open
```{r load-data}
data_trn <- read_csv(here::here(path_data, "ames_train_cln.csv"),
              show_col_types = FALSE) 
data_val <- read_csv(here::here(path_data, "ames_val_cln.csv"),
              show_col_types = FALSE) 
data_test <- read_csv(here::here(path_data, "ames_test_cln.csv"),
              show_col_types = FALSE)

glimpse(data_trn)
glimpse(data_val)
glimpse(data_test)
```

set appropriate variable classes (remember nominal and ordinal variables should be factors!)
```{r}
class_ames <- function(df){
  suppressWarnings(
    df |> 
      mutate(across(where(is.character), factor)) |> 
      mutate(overall_qual = factor(overall_qual, levels = 1:10), 
             garage_qual = fct_relevel(garage_qual, 
                                       c("no_garage", "po", "fa", 
                                         "ta", "gd", "ex")))
  )
}

data_trn <- class_ames(data_trn)
data_val <- class_ames(data_val)
data_test <- class_ames(data_test)

str(data_trn)
str(data_val)
str(data_test)
```


### Create tracking tibble

Create a tibble to track the validation errors across various model configurations.
```{r}
error_val <- tibble(model = character(), rmse_val = numeric()) |> 
  glimpse()
```

## LM Model building instructions

You may consider as many LM configurations as you wish, but there are a few rules:

* All models must eventually predict **raw** sale_price (advanced hint: you may want to transform the outcome, but then if you do, make sure you transform the predictions back to raw $ when you evaluate the RMSE and when you submit model predictions; and if that does not make sense, don't worry about it.  Just work with raw dollars!)

* You must build a minimum of 3 models
* At least one model must include a transformation of a numeric predictor
* At least one model must include a categorical predictor with more than two levels
* At least one model must include a categorical variable with levels that have been modified during feature engineering (NOT including dummy coding or simple transformation to numeric)
* At least one model must include an interaction

Your models may include as many other predictors as you would like (e.g. your model satisfying the transformed numeric predictor requirement may also include categorical variables, numeric predictors without transformations, etc.). Be as creative as you'd like!


## LM 1

List out which required model components you are including for your first model (delete ones that do not apply):
*Lot area*
*Neighborhood*
*Garage_qual*
*Gr_liv_area x Neighborhood*

### Set up recipe

Informed by EDA from your modeling EDA script, create a recipe for your first model.
```{r}
m_1 <- recipe(sale_price ~ lot_area + neighborhood + garage_qual + gr_liv_area + neighborhood, 
                     data = data_trn) |> 
  step_interact(~ gr_liv_area:neighborhood)

m_1
```

### Training feature matrix

Use the `prep()` function to prep your recipe
```{r}
m_1 <- m_1 |> 
  prep(training = data_trn)

m_1
```


Use the `bake()` function to generate the feature matrix of the training data using `rec_prep`
```{r}
train_matrix <- bake(m_1, new_data = NULL)

head(train_matrix)
```

### Fit your model

Fit a linear model predicting `sale_price` from your training feature matrix
```{r}
lm_model1 <- lm(sale_price ~ ., data = train_matrix)

summary(lm_model1)
```

### Validation feature matrix

Use `bake()` to generate the feature matrix of the validation data that we will use to assess your model.

**My validation data was not matching up with my training data due to what I suspect is extra factors in the training set compared to the validation set**
```{r}
data_val$neighborhood <- factor(data_val$neighborhood, levels = levels(data_trn$neighborhood))
val_matrix1 <- bake(m_1, new_data = data_val)

glimpse(val_matrix1)
```

### Assess your model

Use the `rmse_vec()` function to calculate the validation error (RMSE) of your model. Add this value to your validation error tracking tibble 
```{r}
pred_val1 <- predict(lm_model1, newdata = val_matrix1)

rmse_val <- rmse_vec(truth = val_matrix1$sale_price, estimate = pred_val1)

error_val <- error_val |> 
  add_row(model = "lm_model1", rmse_val = rmse_val)

error_val
```

### Visualize performance

Visualize the relationship between raw and predicted sale price in your validation set using the `plot_truth()` function.
```{r}
plot_truth(truth = val_matrix1$sale_price, estimate = pred_val1)
```

## LM 2

List out which required model components you are including for your next model (delete ones that do not apply):
*Year_built*
*Overall_qual*
*Bldg_type*
*log(gr_liv_area) x overall_qual*

### Set up recipe

Informed by EDA from your modeling EDA script, create a recipe for your next model.
```{r}
m_2 <- recipe(sale_price ~ year_built + overall_qual + bldg_type + gr_liv_area + overall_qual, 
                     data = data_trn) |> 
  step_log(gr_liv_area) |> 
  
  step_interact(~ gr_liv_area:overall_qual)

m_2
```

### Training feature matrix

Use the `prep()` function to prep your recipe
```{r}
m_2 <- m_2 |> 
  prep(training = data_trn)

m_2
```


Use the `bake()` function to generate the feature matrix of the training data.
```{r}
train_matrix2 <- bake(m_2, new_data = NULL)

head(train_matrix2)
```

### Fit your model

Fit a linear model predicting raw `sale_price` from your training feature matrix.
```{r}
lm_model2 <- lm(sale_price ~ ., data = train_matrix2)

summary(lm_model2)
```

### Validation feature matrix

Use `bake()` to generate the feature matrix of the validation data that we will use to assess your model.
```{r}
val_matrix2 <- bake(m_2, new_data = data_val)

head(val_matrix2)
```

### Assess your model

Use `rmse_vec()` to calculate the validation error (RMSE) of your model. Add this value to your validation error tracking tibble.

```{r}
val_pred_2 <- predict(lm_model2, newdata = val_matrix2)

rmse_val_2 <- yardstick::rmse_vec(truth = data_val$sale_price, estimate = val_pred_2)

rmse_val_2
```

### Visualize performance

Visualize the relationship between raw and predicted sale price in your validation set using the plot_truth function.
```{r}
plot_truth(data_val$sale_price, val_pred_2)
```

## LM 3

List out which required model components you are including for your next model (delete ones that do not apply):
*Log(gr_liv_area)*
*Neighborhood*
*Overall_qual*
*Bldg_type x year_built*

### Set up recipe

Informed by EDA from your modeling EDA script, create a recipe for your next model.
```{r}
m_3 <- recipe(sale_price ~ gr_liv_area + neighborhood + overall_qual + bldg_type + year_built,
              data = data_trn) |> 
  step_interact(~ bldg_type:year_built)  

m_3
```

### Training feature matrix

Use the `prep()` function to prep your recipe
```{r}
m_3 <- m_3 |> 
  prep(training = data_trn)

m_3
```


Use the `bake()` function to generate the feature matrix of the training data.
```{r}
train_matrix3 <- bake(m_3, new_data = NULL)

head(train_matrix3)
```


### Fit your model

Fit a linear model predicting raw `sale_price` from your training feature matrix.
```{r}
lm_model3 <- lm(sale_price ~ ., data = train_matrix3)

summary(lm_model3)
```

### Validation feature matrix

Use `bake()` to generate the feature matrix of the validation data that we will use to assess your model.
```{r}
val_matrix3 <- bake(m_3, new_data = data_val)

head(val_matrix3)
```

### Assess your model

Use `rmse_vec()` to calculate the validation error (RMSE) of your model. Add this value to your validation error tracking tibble.
```{r}
val_pred_3 <- predict(lm_model3, newdata = val_matrix3)

rmse_val_3 <- yardstick::rmse_vec(truth = data_val$sale_price, estimate = val_pred_3)

rmse_val_3
```

### Visualize performance

Visualize the relationship between raw and predicted sale price in your validation set using the plot_truth function.
```{r}
plot_truth(truth = val_matrix3$sale_price, estimate = val_pred_3)
```

## Additional configurations

Create as many code chunks as you would like below to test additional linear model configurations. Record your models' performance in your validation error tracking tibble.

```{r}
data_trn$garage_area[data_trn$garage_area == 0] <- 1

m_4 <- recipe(sale_price ~ gr_liv_area + garage_area + lot_area + neighborhood + overall_qual, 
              data = data_trn) |> 
  step_log(gr_liv_area, garage_area, lot_area) |>  
  step_log(sale_price) |>  
  step_interact(~ gr_liv_area:neighborhood)

```

```{r}
m_4 <- m_4 |> 
  prep(training = data_trn)

m_4
```

```{r}
train_matrix4 <- bake(m_4, new_data = NULL)

head(train_matrix4)
```

```{r}
lm_model4 <- lm(sale_price ~ ., data = train_matrix4)

summary(lm_model4)
```

```{r}
val_matrix4 <- bake(m_4, new_data = data_val)

head(val_matrix4)
```

```{r}
val_pred_4 <- predict(lm_model4, newdata = val_matrix4)

rmse_val_4 <- yardstick::rmse_vec(truth = data_val$sale_price, estimate = val_pred_4)

rmse_val_4
```

```{r}
plot_truth(truth = val_matrix4$sale_price, estimate = val_pred_4)
```

I literally think this model looks crazy and therefore, is not a good model. I'm not entirely sure what I did here. 

## Predictions

This section is for generating predictions for your best model in the held out test set. **You  should only generate predictions for ONE model out of all `lm` and `knn` models you fit across both homework `.qmd` files.** The TAs will use these predictions to generate your ONE estimate of model performance in the held out data. (Remember the stakes, people.)

### Assign best model

If your **best** model is a `lm` from this script, edit the following objects in the code chunk below:

* Assign the model object from your best model to `best_model`
* Copy and paste the recipe for your best model (created from the training data) to assign to `best_rec` below.
* Type your last name between the quotation marks of `last_name` (e.g. "santana") for file naming.

If your best model is a `knn`, don't edit anything in the next two code chunks and skip to the final section to save, render, etc.
```{r assign-best}
best_model <- lm_model3

best_rec <- recipe(sale_price ~ gr_liv_area + neighborhood + overall_qual + bldg_type + year_built,
                   data = data_trn) |> 
  step_interact(~ bldg_type:year_built)  

last_name <- "Friedl"
```

### Generate test predictions

Run this code chunk to save out your best model's predictions of raw `sale_price` in the held-out test set. Look over your predictions to confirm your model generated valid predictions for each test observation.
(If you left `best_model` as NA above, no file will be saved out).

ADVANCED NOTE:  You *could* improve this code below to train an even better model.  If you know what could be improved, do it.  If not, do not worry. This will work well enough (but maybe not for a free lunch!!).  

ADVANCED NOTE 2: If your model used a transformation of `sale_price` you will also need to update this code.

Had issues w/ code provided to get csv to show. 
```{pred-test}
if (!is.null(best_model)){
  
  rec_prep <- best_rec |> 
    prep(training = data_trn)
  
  feat_test <- rec_prep |> 
    bake(new_data = data_test)
  
  test_preds <- data_test |> 
  mutate(sale_price = {
    pred <- predict(best_model, feat_test)
    if (is.vector(pred)) pred else pred$.pred
  }) |> 
  select(pid, sale_price)
  
   write.csv(test_preds, here::here(path_data,str_c("test_preds_", 
                                                   last_name,
                                                   ".csv")))
}
```


## Save, knit, upload, and remember

### Save
Save this `.qmd` file with your last name at the end (e.g. `hw_unit_3_lm_santana.qmd`)

### Knit
Knit the file to html and upload your knit html file to Canvas.

### Upload
Upload your saved `test_preds_(lastname).csv` (if you created it in this script) to Canvas. We will assess everyone's predictions in the test set. **The winner gets free lunch!!!!**

### Remember...there's more
Don't forget to complete the `knn` portion of this assignment, `hw_unit_3_knn.qmd` as well.

**~ ~ ~ You are a machine learning superstar - and we are proud of you ~ ~ ~**
