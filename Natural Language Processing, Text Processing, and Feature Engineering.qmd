---
title: 'Unit 12: Natural Language Processing, Text Processing, and Feature Engineering'
author: "Nicole Friedl"
date: "`r lubridate::today()`"
format: 
  html: 
    embed-resources: true
    toc: true 
    toc_depth: 4
editor_options: 
  chunk_output_type: console
---

## Assignment overview

To begin, download the following from the course web book (Unit 12):

* `hw_unit_12_nlp.qmd` (notebook for this assignment)

* `alcohol_tweets.csv` ~5k public Tweets collected from people in New York State from July 2013 - July 2014. Tweets were filtered to contain drinking related keywords (e.g., drunk, beer, party), and were labeled by Amazon MTurk workers to identify tweets that were about the user drinking alcohol. The data set contains the following variables:
  * `tweet_id`: unique Twitter id of the user
  * `user_drinking`: labeled yes/no if the tweet is about the user drinking
  * `text`: The raw text of the tweet. Note: raw text is listed as NA in this dataset if the tweet only contained an image or gif (i.e., no text was present)

* `glove_twitter.csv`: These are GloVe vectors - pretrained semantic embeddings (i.e., features of word meaning) derived from Twitter data by a group from [Stanford](https://nlp.stanford.edu/projects/glove/). We provide you with a CSV version of one of the representation sets, but you can look at the other data and related tutorial on Stanford's website. They have vectors of varying sizes, generated from different large language corpora. These are very useful for examining language semantics.

* A caution about these Twitter data: these are raw language data from Twitter's platform. We have not curated the data in any way. As a result, the text includes content that some may well find offensive and users of these data should take note. Words used and topics discussed could be harsh, offensive, or inflammatory. Foul language is certainly present.

Your goal is to build the best logistic regression model that you can to predict whether a tweet is about a user drinking alcohol (or not). Similar to the neural networks homework, you will have a lot of flexibility in how you approach this assignment. We will include minimum steps you must consider for model building, but feel free to expand EDA beyond the steps listed in order to build the best model you can.

Your assignment is due Wednesday at 8:00 PM. Let's get started!

-----

## Setup

Load packages, paths, and function scripts you may need, including parallel processing code.
```{r}
options(conflicts.policy = "depends.ok")
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true")
tidymodels_conflictRules()

library(tidyverse)
library(tidymodels)
library(Matrix, exclude = c("expand", "pack", "unpack"))
library(magrittr, exclude = c("set_names", "extract"))
library(rsample)
library(parsnip)
library(recipes)
library(workflows)
library(stringr)

devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_plots.R?raw=true")
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_eda.R?raw=true")
options(tibble.width = Inf, dplyr.print_max=Inf)
rerun_setting <- FALSE

cl <- parallel::makePSOCKcluster(parallel::detectCores(logical = FALSE))
doParallel::registerDoParallel(cl)

path_data <- "homework/data"
```

## Part 1: Read in and split data

Since we do not have that much data, we will be splitting into a validation set for considering model configurations. However, you should still only look at the training portion of this split during EDA.

### Read in `alcohol_tweets.csv`
```{r}
data <- read_csv(here::here(path_data, "alcohol_tweets.csv"),
                     show_col_types = FALSE) |> 
  glimpse()
```

For this assignment it is helpful to see (and potentially copy) the `id` values from tweets, which we can't do easily if the number is in scientific notation. To change this in R, set the relevant option called `scipen`. You can do this by running the code below, which says that any integer with 20 decimal places or fewer, will be in raw integer form, not scientific notation. This can be useful with ID-type variables like we use here.

```{r}
options(scipen = 20)
```

### Glove embeddings
The file containing the GloVe embeddings is very large, so we will use `fread()`.
```{r}
glove_embeddings <- data.table::fread(here::here(path_data, 'glove_twitter.csv')) |> 
  as_tibble()
```


### Validation splits

Use the provided splits across all model configurations you consider

```{r}
set.seed(12345)

splits <- data |> 
  validation_split(strata = "user_drinking")

# Pull out indices of training data
training_ind <- splits |> 
  unlist(recursive = FALSE)

training_ind <- training_ind$splits$in_id

data_train <- data |> 
  slice(training_ind)
```

## Part 1: Cleaning EDA

In this section, use the tidytext package to get a better sense of the data to guide model building in Part 2. Use data_train to identify what cleaning steps you may want to take. We will apply your identified cleaning steps to the full dataset and resplit again before building models.

### Initial Cleaning

At a minimum, complete the following steps:

* Clean the tweets. Visual inspection of text data is really important. Are there any special characters or parsing errors that need to be handled in the text? For example, how will you handle NA tweets that were originally just images? This text is likely to have other characters that you want to consider for modeling the outcome. Take steps to process the text in a way that serves your modeling objective.

* Classing variables. Check if `id` and `user_drinking` variables are the correct class. What is the distribution of the outcome?

* Tokenization. Tokenize your text into both unigrams and bigrams. The help page for `unnest_tokens()` can help you understand your options for tokenization in tidytext.

* Stopwords. Load the stopwords that you plan to use in your workflow. Think about which stopwords you will use and why, and have those ready. Look at the tokenized data in order to consider how stopwords will impact modeling.

```{r}
library(tidytext)

clean_tweets <- function(text) {
  text <- ifelse(is.na(text), "", text)
  
  text |> 
    str_to_lower() |> 
    str_replace_all("http\\S+|www\\S+", "") |> 
    str_replace_all("@\\w+", "") |> 
    str_replace_all("#(\\w+)", "\\1") |> 
    str_replace_all("^rt:", "") |> 
    str_replace_all("[[:punct:]]", " ") |> 
    str_replace_all("\\d+", "") |> 
    str_squish()
}

data <- data |> 
  mutate(clean_text = clean_tweets(text))

data |> 
  select(text, clean_text) |> 
  head(5)
```

```{r}
unigrams <- data |> 
  unnest_tokens(word, clean_text, token = "words") |> 
  filter(word != "")  

unigrams |> 
  count(word, sort = TRUE) |> 
  head(20)

bigrams <- data |> 
  unnest_tokens(bigram, clean_text, token = "ngrams", n = 2) |> 
  filter(!is.na(bigram))  

bigrams |> 
  count(bigram, sort = TRUE) |> 
  head(20)
```

```{r}
data("stop_words")

custom_stopwords <- tribble(
  ~word, ~lexicon,
  "rt", "custom",
  "amp", "custom",
  "https", "custom",
  "http", "custom"
)

all_stopwords <- bind_rows(stop_words, custom_stopwords)

unigrams_filtered <- unigrams |> 
  anti_join(all_stopwords, by = "word")

unigrams_filtered |> 
  count(word, sort = TRUE) |> 
  head(20)
```

### Explore tokens

At a minimum, complete the following steps (for both unigram and bigram tokens):

* Display the total number of tokens used across all tweets

* Display the total number of unique tokens used across all tweets

* Plot the frequency distribution of the 1000 most common tokens

* Review the top 1000 tokens
```{r}
library(ggplot2)

total_unigrams <- nrow(unigrams)
unique_unigrams <- unigrams |> 
  distinct(word) |> 
  nrow()

cat("Total unigram tokens:", total_unigrams, "\n")
cat("Unique unigram tokens:", unique_unigrams, "\n")

top_unigrams <- unigrams |> 
  count(word, sort = TRUE) |> 
  head(1000)

ggplot(top_unigrams, aes(x = reorder(word, n), y = n)) +
  geom_col() +
  coord_flip() +
  labs(x = "Word", y = "Count", title = "Frequency of Top 1000 Unigrams") +
  theme_minimal() +
  theme(axis.text.y = element_text(size = 6))  

print("Top 20 unigrams:")
top_unigrams |>  head(20)

total_bigrams <- nrow(bigrams)
unique_bigrams <- bigrams |> 
  distinct(bigram) |> 
  nrow()

cat("Total bigram tokens:", total_bigrams, "\n")
cat("Unique bigram tokens:", unique_bigrams, "\n")

top_bigrams <- bigrams |> 
  count(bigram, sort = TRUE) |> 
  head(1000)

ggplot(top_bigrams, aes(x = reorder(bigram, n), y = n)) +
  geom_col() +
  coord_flip() +
  labs(x = "Bigram", y = "Count", title = "Frequency of Top 1000 Bigrams") +
  theme_minimal() +
  theme(axis.text.y = element_text(size = 6))  

print("Top 20 bigrams:")
top_bigrams |>  head(20)
```


### Final cleaning

Based on your EDA above, apply your desired cleaning steps to your full data set. Call this "cleaned_data".
```{r}
cleaned_data <- data |> 
  mutate(user_drinking = factor(user_drinking, levels = c("no", "yes"))) |> 
  select(id, user_drinking, text, clean_text)

glimpse(cleaned_data)
```

### Resplit
Run this code chunk to apply the same split as we did at the beginning to the fully cleaned data
```{r}

set.seed(12345)

splits <- cleaned_data |> 
  validation_split(strata = "user_drinking")

```

## Part 2: Model Building

Now you will consider multiple model configurations to predict `user_drinking` from the tweet text!

Each model you train must:

* Use the provided validation splits provided below

* Be a glmnet

* Use balanced accuracy as the metric

* Specify all tweet cleaning/feature engineering steps using `tidytext` recipes

At a minimum, fit at least 3 model configurations:

* A Unigram BoW model

* An n-gram BoW model (using either bigrams, combination of unigrams and bigrams, etc.)

* A model using pretrained Twitter embeddings (provided in homework files in web book, you can use `data.table::fread()` to open this file)

You may end up considering more than just three model configurations. Across these configurations, also consider:

* Impact of different cleaning steps on different models

* Number of features retained in your document term matrix

* The stopwords that are important for these data in particular

* Stemming

Let's do it!

### Consider Configurations

Use as many chunks as you'd like below to consider model configurations. Save the resampled performance for each configuration you consider.
```{r}
#Practicing helper functions

evaluate_model <- function(wf, splits) {
  train_data <- splits$splits[[1]]$data[splits$splits[[1]]$in_id, ]

  val_indices <- setdiff(1:nrow(splits$splits[[1]]$data), splits$splits[[1]]$in_id)
  val_data <- splits$splits[[1]]$data[val_indices, ]
  
  wf_fit <- wf |> fit(train_data)
  
  class_preds <- wf_fit |> predict(val_data)
  prob_preds <- wf_fit |> predict(val_data, type = "prob")
  
  results <- bind_cols(
    class_preds,
    prob_preds,
    val_data |> select(user_drinking)
  )
  
  metrics <- yardstick::bal_accuracy(
    results,
    truth = user_drinking,
    estimate = .pred_class  
  )
  
  return(metrics)
}
```

```{r}
# Unigram boW model
library(textrecipes)
library(rsample)
library(stopwords)

# Alternative approach using training() function
train_data <- training(splits$splits[[1]])

# Then create the recipe
unigram_recipe <- recipe(user_drinking ~ clean_text, data = train_data) |> 
  step_tokenize(clean_text) |> 
  step_stopwords(clean_text) |> 
  step_tokenfilter(clean_text, min_times = 5) |> 
  step_tokenfilter(clean_text, max_tokens = 100) |> 
  step_tfidf(clean_text)

glmnet_spec <- logistic_reg(penalty = 0.01, mixture = 1) |> 
  set_engine("glmnet") |> 
  set_mode("classification")

unigram_wf <- workflow() |> 
  add_recipe(unigram_recipe) |> 
  add_model(glmnet_spec)

unigram_metrics <- evaluate_model(unigram_wf, splits)
print("Unigram Model Performance:")
unigram_metrics
```

```{r}
# Unigrams and Bigrams
train_data <- splits$splits[[1]]$data[splits$splits[[1]]$in_id, ]

ngram_recipe <- recipe(user_drinking ~ clean_text, data = train_data) |> 
  step_tokenize(clean_text, 
                engine = "tokenizers", token = "words") |> 
  step_stopwords(clean_text) |> 
  step_ngram(clean_text, num_tokens = 2, min_num_tokens = 1) |> 
  step_tokenfilter(clean_text, min_times = 5) |> 
  step_tokenfilter(clean_text, max_tokens = 100) |> 
  step_tfidf(clean_text) |> 
  step_normalize(all_predictors())

ngram_wf <- workflow() |> 
  add_recipe(ngram_recipe) |> 
  add_model(glmnet_spec)

ngram_metrics <- evaluate_model(ngram_wf, splits)
print("Ngram Model Performance:")
ngram_metrics
```

```{r}
# Stem
stemmed_recipe <- recipe(user_drinking ~ clean_text, data = train_data) |> 
  step_tokenize(clean_text, 
                engine = "tokenizers", token = "words") |> 
  step_stopwords(clean_text) |> 
  step_stem(clean_text) |> 
  step_tokenfilter(clean_text, min_times = 5) |> 
  step_tokenfilter(clean_text, max_tokens = 1000) |> 
  step_tfidf(clean_text) |> 
  step_normalize(all_predictors())

stemmed_wf <- workflow() |> 
  add_recipe(stemmed_recipe) |> 
  add_model(glmnet_spec)

stem_metrics <- evaluate_model(stemmed_wf, splits)
print("Stem Model Performance:")
stem_metrics
```

## Part 3: Best Model

Since we did NOT hold out an independent test set and selected our model configuration based on cross validated performance, these next steps are subject to some degree of optimization bias!

### Print the cross-validated performance of your best performing model
```{r}
model_metrics <- tibble(
  Model = c("Unigram BoW", "N-gram", "Stemmed"),
  Balanced_Accuracy = c(
    unigram_metrics$.estimate,
    ngram_metrics$.estimate,
    stem_metrics$.estimate
  )
)

model_metrics |>
  arrange(desc(Balanced_Accuracy))

best_model_name <- model_metrics |>
  arrange(desc(Balanced_Accuracy)) |>
  slice(1) |>
  pull(Model)

cat("The best performing model is:", best_model_name, "\n")

best_wf <- if(best_model_name == "Unigram BoW") {
  unigram_wf
} else if(best_model_name == "N-gram") {
  ngram_wf
} else {
  stemmed_wf
}
```

### Train your top performing model on the full data set
```{r}
best_fit <- fit(ngram_wf, data = cleaned_data)

best_fit_model <- best_fit |> extract_fit_parsnip()

tidy(best_fit_model) |> 
  arrange(desc(abs(estimate))) |> 
  slice_head(n = 20)
```

### Plot variable importance scores of your top performing model
* Do these make sense to you? Why or why not?

> This shows the top 20 most important variables identified by the N-gram model, ranked by their importance score, which suggests they are strong predictors in the model's task.
```{r}
library(vip)

best_fit |>
  extract_fit_parsnip() |>
  vip(num_features = 20) +
  labs(title = paste("Top 20 Important Variables in", best_model_name, "Model"))

if (FALSE) {  
  coefs <- best_fit |>
    extract_fit_parsnip() |>
    tidy() |>
    filter(estimate != 0) |>
    arrange(desc(abs(estimate)))
  
  coefs |>
    head(20) |>
    mutate(term = forcats::fct_reorder(term, abs(estimate))) |>
    ggplot(aes(x = term, y = estimate, fill = estimate > 0)) +
    geom_col() +
    coord_flip() +
    labs(
      title = paste("Top 20 Important Variables in", best_model_name, "Model"),
      x = "Term",
      y = "Coefficient Estimate",
      fill = "Positive Effect"
    ) +
    theme_minimal()
}

```

### Make a confusion matrix of your model's predictions
* What do you notice about the predictions that your model is making?

> Matrix shows that the N-gram model has some predictive power, but it's clearly biased towards predicting "no".  The model struggles with identifying "yes" instances, indicating a need for improvement. 
```{r}
predictions <- best_fit |>
  predict(cleaned_data) |>
  bind_cols(
    best_fit |> predict(cleaned_data, type = "prob"),
    cleaned_data |> select(user_drinking)
  )

conf_mat <- predictions |>
  conf_mat(truth = user_drinking, estimate = .pred_class)

conf_mat

conf_mat |>
  autoplot(type = "heatmap") +
  labs(title = paste("Confusion Matrix for", best_model_name, "Model"))
```


**Knit this file and submit your knitted html. Make sure to leave yourself enough time to knit. Nice job completing this assignment - we are proud of you.**
