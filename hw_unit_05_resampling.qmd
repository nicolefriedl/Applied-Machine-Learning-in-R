---
title: "Homework Unit 5: Resampling"
author: "Nicole Friedl"
date: "`r lubridate::today()`"
format: 
  html: 
    embed-resources: true
    toc: true 
    toc_depth: 4
editor_options: 
  chunk_output_type: console
---   

## Introduction

To begin, download the following from the course web book (Unit 5):

- `hw_unit_05_resampling.qmd` (notebook for this assignment)

- `smoking_ema.csv` (data for this assignment)

- `data_dictionary.csv` (a data dictionary for the `smoking_ema.csv` dataset)

The data for this week's assignment contains self-report ecological momentary assessment (EMA) data from a cigarette smoking cessation attempt study. These are real (de-identified) data from our laboratory! We've tried to structure the assignment so that you don't really need to understand the data, but looking quickly at the data dictionary may be helpful. Briefly, the outcome variable is `cigs` (whether a cigarette has been smoked since previous report). The `cigs` variable comes from the report *following* the report from which all other data come. In other words, all predictor variables come from time 1, and the outcome variable (`cigs`) comes from time 2. Predictor variables include things like current craving, several measures of affect, the intensity of any stressful events, and the random treatment assignment (active "nrt" treatment, or "placebo").     

The data are already cleaned, and you don't need to do any modeling EDA. Of course you **normally** would do modeling EDA before fitting any models, but we're trying to keep the assignment from getting too long!      

In this assignment, you will practice selecting among model configurations using resampling methods. This includes training models with 2 different feature sets (specified in your recipe) and tuning hyperparameters (`k` in KNN). As you'll see when you load the data file, there are a lot of observations! This fact combined with the more complex resampling procedures you'll be using in this assignment means that **fitting models may take a little longer**. We've kept the active coding component of the assignment to a reasonable length, but the run time may be a little longer, so please try to plan for this!    

Let's get started!

----- 

## Setup

### Handle conflicts
```{r}
#| message: false

options(conflicts.policy = "depends.ok")
library(conflicted)
conflict_prefer("%>%", "dplyr")  # Resolve conflict for the pipe operator
conflict_prefer("discard", "purrr")  # Ensure purrr::discard is used
conflict_prefer("col_factor", "scales")  # Resolve conflict with scales
# Resolve the conflict for the 'fixed' function
conflict_prefer("fixed", "stringr")
```

### Load required packages 
```{r}
library(tidyverse)
library(tidymodels)
library(yardstick)
library(tune)
library(rsample)
library(parsnip)
library(recipes)
```

### Source function scripts (John's or your own)
```{r}
devtools::source_url("https://github.com/jjcurtin/lab_support/blob/main/fun_ml.R?raw=true")
tidymodels_conflictRules()
```

### Specify other global settings
If you are going to use `cache_rds(), you might include `rerun_setting <- FALSE` in this chunk
```{r}
rerun_setting <- FALSE
cl <- parallel::makePSOCKcluster(parallel::detectCores(logical = FALSE))
doParallel::registerDoParallel(cl)
```

### Paths
```{r}
path_data <- "homework/data"
```

### Read in data

Read in the `smoking_ema.csv` data file   
Use `here` and a relative path for your data.  Make sure your iaml project is open.
```{r}
data <- read_csv(here::here(path_data, "smoking_ema.csv"),
              show_col_types = FALSE) 
```

## Part 1: Selecting among model configurations with k-fold cross-validation

In this portion of the assignment, you will be using k-fold cross-validation to select among two different model configurations (Feature set 1 and Feature set 2). These model configurations will differ only on feature set (no hyperparameter tuning yet!). They will also both use LM logistic regression as the statistical algorithm. At this point, you will ignore the fact that there are repeated observations, grouped by subid until we tell you to address that.

### Split data

Split your data into 10 folds (`k = 10`; `repeats = 1`) stratifying on the outcome (`cigs`)
```{r}
data <- data |> 
  mutate(cigs = factor(ifelse(cigs == 0, "no", "yes"), levels = c("no", "yes")))

set.seed(01131997)

splits_kfold <- vfold_cv(data, v = 10, strata = "cigs", repeats = 1)

splits_kfold
```

### Feature Set 1

Build a recipe with the following specifications: 

- Use `cigs` as the outcome variable regressed on `craving`.  
- Set up the outcome as a factor with the positive class ("yes") as the second level.
- Use craving as a predictor by simply transforming this ordinal variable to numeric based on the order of its scores.

```{r}
rec_kfold_1 <- recipe(cigs ~ craving, data = data) |> 
  step_mutate(craving = as.numeric(craving))
```

Fit the model with k-fold cross-validation. Use logistic regression as your statistical algorithm, `rec_kfold_1` as your recipe, and `accuracy` as your metric.
```{r}
fits_kfold_1 <- logistic_reg() %>%
  set_engine("glm") %>%
  fit_resamples(
    preprocessor = rec_kfold_1,
    resamples = splits_kfold,
    metrics = metric_set(accuracy)
  )
```

Examine performance estimates. Use the `collect_metrics()` function to make a table of the held-out performance estimates from the 10 folds
```{r}
metrics_kfold_1 <- collect_metrics(fits_kfold_1, summarize = FALSE)
metrics_kfold_1
```

Plot a histogram of the performance estimates
```{r}
ggplot(metrics_kfold_1, aes(x = .estimate)) +
  geom_histogram(bins = 10, fill = "lightblue", color = "black") +
  labs(title = "Histogram of Accuracy Estimates (Feature Set 1)",
       x = "Accuracy",
       y = "Frequency")
```

Print the average performance over folds with the `summarize = TRUE` argument.
```{r}
collect_metrics(fits_kfold_1, summarize = TRUE)
```

### Feature Set 2

Build a second recipe with the following specifications:   

- Use `cigs` as the outcome variable regressed on all variables.  
- Set up the outcome as a factor with the positive class ("yes") as the second level.
- Set up other variables as factors where needed.
- Apply dummy coding.
- Create an interaction between `stressful_event` and `lag`.
- Remove the `subid` variable with `step_rm()`

```{r}
data <- data %>%
  mutate(cigs = factor(ifelse(cigs == 0, "no", "yes"), levels = c("no", "yes")))

rec_kfold_2 <- recipe(cigs ~ ., data = data) %>%
  step_rm(subid) %>%
  step_interact(terms = ~ stressful_event:lag) %>% 
  step_mutate(across(c(anxious, irritable, sad, happy, stressful_event, positive_event, txt), as.factor)) %>%
  step_dummy(all_nominal_predictors())
```

Fit the model with k-fold cross-validation. Use logistic regression as your statistical algorithm, `rec_kfold_2` as your recipe, and `accuracy` as your metric.
```{r}
fits_kfold_2 <- logistic_reg() %>%
  set_engine("glm") %>%
  fit_resamples(
    preprocessor = rec_kfold_2,
    resamples = splits_kfold,
    metrics = metric_set(accuracy)
  )
```

Examine performance estimates. Use the `collect_metrics()` function to make a table of the held-out performance estimates.
```{r}
metrics_kfold_2 <- collect_metrics(fits_kfold_2, summarize = FALSE)
metrics_kfold_2
```

Plot a histogram of the performance estimates.
```{r}
ggplot(metrics_kfold_2, aes(x = .estimate)) +
  geom_histogram(bins = 10, fill = "lightblue", color = "black") +
  labs(title = "Histogram of Accuracy Estimates (Feature Set 2)",
       x = "Accuracy",
       y = "Frequency")
```

Print the average performance over folds with the `summarize = TRUE` argument.
```{r}
collect_metrics(fits_kfold_2, summarize = TRUE)
```

### Select the best model configuration

Look back at your two average performance estimates (one from each feature set). Which model configuration would you select? Why? Type your response between the asterisks below.     

*Feature 2 is better. Feature Set 2 has a higher average accuracy (0.858) compared to Feature Set 1 (0.817). This indicates that, on average, the model with Feature Set 2 performs better in predicting the outcome (cigs). While feature set one has a much smaller standard error, the difference in mean accuracy is large enough, that feature set 2 is still the better model.*

## Part 2: Tuning hyperparameters with bootstrap resampling

Now we'll use bootstrapping to tune a hyperparameter (`k`) in KNN models. This means that you'll consider multiple values of `k` in a more principled way. You will now be training many more model configurations (differing by feature set and hyperparameter value). You will pass in several hyperparameters for each of the feature set configurations we defined above. You will then use your resampled performance estimates to select the best model configuration (the combination of feature set and hyperparameter value that produces the best performance estimate).

### Split data

Split your data using bootstrap. Stratify on `cigs` and use 100 bootstraps. Don't forget to set a seed!
```{r}
data <- data %>%
  mutate(cigs = factor(ifelse(cigs == 0, "no", "yes"), levels = c("no", "yes")))

set.seed(01131997)
splits_boot <- bootstraps(data, times = 100, strata = "cigs")
```

### Set up hyperparameter grid

Create a tibble with all values of the hyperparameter (`k`) you will consider. Include at least 25 values of `k` - you decide which values!
```{r}
hyper_grid <- tibble(neighbors = seq(1, 51, by = 2)) 
```

### Feature Set 1

Build a recipe with the following specifications: 

- Use `cigs` as the outcome variable regressed on `craving`.  
- Set up the outcome as a factor with the positive class ("yes") as the second level.
- Use the same numeric transformation of craving as before

```{r}
rec_boot_1 <- recipe(cigs ~ craving, data = data) %>%

 step_mutate(craving = as.numeric(craving))
```

Tune the model with bootstrapping for cross-validation. Use KNN classification as your statistical algorithm, `rec_boot_1` as your recipe, `hyper_grid` as your tuning grid, and `accuracy` as your metric.
```{r}
tune_boot_1 <- nearest_neighbor(neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("classification") %>%
  tune_grid(
    preprocessor = rec_boot_1,
    resamples = splits_boot,
    grid = hyper_grid,
    metrics = metric_set(accuracy)
  )
```

Examine performance estimates across the OOB held-out sets. Print the average performance for each configuration with the `collect_metrics()` function and the `summarize = TRUE` argument. Remember that you will now see one estimate per hyperparameter value, averaged across bootstraps.
```{r }
collect_metrics(tune_boot_1, summarize = TRUE)
```

Plot the average performance by hyperparameter value. You can use `plot_hyperparameters()` from `fun_ml.R` or your own code.
```{r}
plot_hyperparameters(tune_boot_1, hp1 = "neighbors", metric = "accuracy")
```

Have you considered a wide enough range of `k` values? How do you know? Type your response between the asterisks. If you didn't use a wide enough range, try again with a wider range

*Yes, I have considered a wider range of k values. The original code used seq(1, 51, by = 2), which covers odd numbers from 1 to 51.  This is a reasonable starting point. In my updated code, I've expanded the range to seq(1, 101, by = 2), covering odd numbers from 1 to 101.  This wider range allows us to see if performance continues to improve or decline beyond k=51*


Print the performance of your best model configuration with the `show_best()` function.
```{r}
show_best(tune_boot_1, metric = "accuracy")
```

### Feature Set 2

Build a second recipe with the following specifications:   

- Use `cigs` as the outcome variable regressed on all variables.  
- Set up the outcome as a factor with the positive class ("yes") as the second level.
- Set up other variables as factors where needed.
- Apply dummy coding.
- Create an interaction between `stressful_event` and `lag`.
- Remove the `subid` variable with `step_rm()`
- **Take appropriate scaling steps for features to be able to fit KNN requirements.**

```{r}
rec_boot_2 <- recipe(cigs ~ ., data = data) %>%
  step_rm(subid) %>%  
  step_dummy(all_nominal(), -all_outcomes()) %>%  
  step_interact(~ stressful_event:lag) %>%  
  step_normalize(all_numeric(), -all_outcomes())  
```

Tune the model with bootstrapping for cross-validation. Use KNN classification as your statistical algorithm, `rec_boot_2` as your recipe, `hyper_grid` as your tuning grid, and `accuracy` as your metric.
```{r}
tune_boot_2 <- nearest_neighbor(neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("classification") %>%
  tune_grid(
    preprocessor = rec_boot_2,
    resamples = splits_boot,
    grid = hyper_grid,
    metrics = metric_set(accuracy)
  )
```

Examine performance estimates. Print the average performance for each configuration with the `collect_metrics()` function and the `summarize = TRUE` argument. Remember that you will now see one estimate per hyperparameter value, averaged across bootstraps.
```{r}
collect_metrics(tune_boot_2, summarize = TRUE)
```

Plot the average performance by hyperparameter value. You can use `plot_hyperparameters()` from `fun_ml.R` or your own code.
```{r}
plot_hyperparameters(tune_boot_2, hp1 = "neighbors", metric = "accuracy")
```

Have you considered a wide enough range of k values? How do you know? Type your response between the asterisks. If you didn't use a wide enough range, try again with a wider range

*I think my range of k values is appropriate.*

Print the performance of your best model configuration with the `show_best()` function.
```{r}
show_best(tune_boot_2, metric = "accuracy")
```

## Part 3: Selecting among model configurations with grouped k-fold cross-validation

Thus far, you've ignored the `subid` variable. However, what that variable reveals is that the many observations within the `smoking_ema.csv` dataset are grouped within 125 unique participants (i.e., repeated observations). In the final part of this assignment, you will use grouped k-fold cross-validation to match the data structure. 

### Split data

Split your data into 10 folds (`k = 10`; `repeats = 1`) using the `group_vfold_cv()` function. Use the grouping argument (`group = "subid"`). For this example, do **not** stratify on `cigs`. Again, don't forget to set a seed!

```{r}
set.seed(01131997)

splits_group_kfold <- group_vfold_cv(data, v = 10, repeats = 1, group = "subid")
```

### Feature Set 1

Fit the first set of models with grouped k-fold cross-validation. Use logistic regression as your statistical algorithm, `rec_kfold_1` as your recipe (doesn't need to change from above!), and `accuracy` as your metric.
```{r}
fits_group_kfold_1 <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification") %>%
  fit_resamples(
    preprocessor = rec_kfold_1,
    resamples = splits_group_kfold,
    metrics = metric_set(accuracy)
  )
```

Examine performance estimates. Use the `collect_metrics()` function to make a table of the held-out performance estimates.
```{r}
metrics_group_kfold_1 <- collect_metrics(fits_group_kfold_1)
metrics_group_kfold_1
```

Plot a histogram of the performance estimates
```{r}
metrics_group_kfold_1 %>%
  ggplot(aes(x = mean)) +
  geom_histogram(binwidth = 0.01, color = "black", fill = "skyblue") +
  labs(title = "Histogram of Accuracy Estimates", x = "Accuracy", y = "Count") +
  theme_minimal()
```

Print the average performance over folds with the `summarize = TRUE` argument.
```{r}
collect_metrics(fits_group_kfold_1, summarize = TRUE)
```

### Feature Set 2

Fit the second model with grouped k-fold cross-validation. Use logistic regression as your statistical algorithm, `rec_kfold_2` as your recipe (doesn't need to change from above!), and `accuracy` as your metric.
```{r}
fits_group_kfold_2 <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification") %>%
  fit_resamples(
    preprocessor = rec_kfold_2,
    resamples = splits_group_kfold,
    metrics = metric_set(accuracy)
  )
```

Examine performance estimates. Use the `collect_metrics()` function to make a table of the held-out performance estimates.
```{r}
metrics_group_kfold_2 <- collect_metrics(fits_group_kfold_2)
metrics_group_kfold_2
```

Plot a histogram of the performance estimates
```{r}
metrics_group_kfold_2 %>%
  ggplot(aes(x = mean)) +
  geom_histogram(binwidth = 0.01, color = "black", fill = "lightgreen") +
  labs(title = "Histogram of Accuracy Estimates (Feature Set 2)", x = "Accuracy", y = "Count") +
  theme_minimal()
```

Print the average performance over folds with the `summarize = TRUE` argument.
```{r}
collect_metrics(fits_group_kfold_2, summarize = TRUE)
```

### Selecting the best model configuration

Look back at your two average performance estimates for each model configurations (feature set 1 and 2) from grouped k-fold cross-validation. Which model configuration would you select? Why? Type your response between the asterisks.

*I would select feature set 1 because I think it looked better when we adjusted our k values compared to the second feature set.*


## Save & render 

Save this .qmd file with your last name at the end (e.g., `hw_unit_5_resampling_wyant`). Make sure you changed "Your name here" at the top of the file to be your own name. Render the file to .html, and upload the html file to Canvas. 

**Way to go!!**